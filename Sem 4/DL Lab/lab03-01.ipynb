{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2431805,"sourceType":"datasetVersion","datasetId":8782}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\nfrom sklearn.model_selection import train_test_split\nimport glob\n\n# Flowers dataset path on Kaggle\nKAGGLE_INPUT = \"/kaggle/input/flowers-recognition/flowers\"\n\n# Create directory structure\nos.makedirs(\"data/train\", exist_ok=True)\nos.makedirs(\"data/val\", exist_ok=True)\n\n# Get all flower classes\nclasses = [d for d in os.listdir(KAGGLE_INPUT) \n           if os.path.isdir(os.path.join(KAGGLE_INPUT, d))]\n\nfor cls in classes:\n    src_folder = os.path.join(KAGGLE_INPUT, cls)\n    \n    os.makedirs(f\"data/train/{cls}\", exist_ok=True)\n    os.makedirs(f\"data/val/{cls}\", exist_ok=True)\n    \n    # Get all images and split 80/20\n    images = glob.glob(f\"{src_folder}/*\")\n    train_imgs, val_imgs = train_test_split(\n        images, test_size=0.2, random_state=42\n    )\n    \n    for img in train_imgs:\n        shutil.copy(img, f\"data/train/{cls}/\")\n    for img in val_imgs:\n        shutil.copy(img, f\"data/val/{cls}/\")\n\nprint(\"✅ Dataset ready!\")\nprint(f\"Classes: {classes}\")\nprint(f\"Train images: {sum(len(os.listdir(f'data/train/{c}')) for c in classes)}\")\nprint(f\"Val images: {sum(len(os.listdir(f'data/val/{c}')) for c in classes)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:52:30.407236Z","iopub.execute_input":"2026-01-28T09:52:30.407638Z","iopub.status.idle":"2026-01-28T09:52:39.014155Z","shell.execute_reply.started":"2026-01-28T09:52:30.407609Z","shell.execute_reply":"2026-01-28T09:52:39.013347Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset ready!\nClasses: ['dandelion', 'daisy', 'sunflower', 'tulip', 'rose']\nTrain images: 3452\nVal images: 865\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimg_size = (224, 224)\nnum_classes = 5  # your dataset\n# Data pipeline (example – adapt to your folders)\ntrain_ds = keras.preprocessing.image_dataset_from_directory(\n    \"data/train\",\n    image_size=img_size,\n    batch_size=32\n)\nval_ds = keras.preprocessing.image_dataset_from_directory(\n    \"data/val\",\n    image_size=img_size,\n    batch_size=32\n)\n# Data augmentation\ndata_augmentation = keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n])\n# Stage 1: freeze base\nbase_model = keras.applications.ResNet50(\n    include_top=False,\n    weights=\"imagenet\",\n    input_shape=img_size + (3,)\n)\nbase_model.trainable = False\ninputs = keras.Input(shape=img_size + (3,))\nx = data_augmentation(inputs)\nx = keras.applications.resnet50.preprocess_input(x)\nx = base_model(x, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(256, activation=\"relu\")(x)\noutputs = layers.Dense(num_classes, activation=\"softmax\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-3),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\nmodel.fit(train_ds, epochs=5, validation_data=val_ds)\n# Stage 2: unfreeze top block \nbase_model.trainable = True\nfor layer in base_model.layers[:-30]:\n    layer.trainable = False\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-4),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\nmodel.fit(train_ds, epochs=5, validation_data=val_ds)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:52:39.015948Z","iopub.execute_input":"2026-01-28T09:52:39.016239Z","iopub.status.idle":"2026-01-28T09:56:38.208868Z","shell.execute_reply.started":"2026-01-28T09:52:39.016217Z","shell.execute_reply":"2026-01-28T09:56:38.208258Z"}},"outputs":[{"name":"stdout","text":"Found 3452 files belonging to 5 classes.\nFound 865 files belonging to 5 classes.\nEpoch 1/5\n\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 203ms/step - accuracy: 0.7500 - loss: 0.7233 - val_accuracy: 0.8902 - val_loss: 0.3044\nEpoch 2/5\n\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 175ms/step - accuracy: 0.9057 - loss: 0.2600 - val_accuracy: 0.8925 - val_loss: 0.3365\nEpoch 3/5\n\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 169ms/step - accuracy: 0.9256 - loss: 0.1974 - val_accuracy: 0.9133 - val_loss: 0.2941\nEpoch 4/5\n\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 169ms/step - accuracy: 0.9378 - loss: 0.1749 - val_accuracy: 0.8971 - val_loss: 0.3212\nEpoch 5/5\n\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 172ms/step - accuracy: 0.9563 - loss: 0.1124 - val_accuracy: 0.9052 - val_loss: 0.3170\nEpoch 1/5\n\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 252ms/step - accuracy: 0.9336 - loss: 0.1927 - val_accuracy: 0.9040 - val_loss: 0.4832\nEpoch 2/5\n\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 228ms/step - accuracy: 0.9760 - loss: 0.0841 - val_accuracy: 0.9237 - val_loss: 0.3156\nEpoch 3/5\n\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 224ms/step - accuracy: 0.9854 - loss: 0.0502 - val_accuracy: 0.9237 - val_loss: 0.3467\nEpoch 4/5\n\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 226ms/step - accuracy: 0.9928 - loss: 0.0282 - val_accuracy: 0.9283 - val_loss: 0.3148\nEpoch 5/5\n\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 227ms/step - accuracy: 0.9926 - loss: 0.0222 - val_accuracy: 0.9191 - val_loss: 0.4040\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7ad41838ccb0>"},"metadata":{}}],"execution_count":4}]}